import pandas as pd
import networkx as nx
import numpy as np
from datetime import datetime, timedelta
import os
from typing import Dict, List, Tuple, Optional, Set
import pickle
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import sqlite3
from functools import lru_cache

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UltraOptimizedGraphGTFS:
    """
    Version am√©lior√©e du graphe GTFS qui utilise SQLite pour avoir de meilleures performances, avec √ßa on a un chargement extr√™mement rapide
    et des requ√™tes bien plus rapides.
    
    Nouvelles optimisations :
    1. Base de donn√©es SQLite pour les requ√™tes rapides
    2. Index spatiaux pour les zones g√©ographiques
    3. Parall√©lisation du traitement
    4. Cache intelligent avec expiration
    5. Pr√©-agr√©gation des donn√©es
    """
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.db_path = f"{data_path}/metro_graph.db"
        self.stops = None
        self.routes = None
        self.transfers = None
        self._load_data_and_setup_db()
        
    def _load_data_and_setup_db(self):
        """Charge les donn√©es et configure la base de donn√©es SQLite"""
        try:
            logger.info("Initialisation de la base de donn√©es ultra-optimis√©e...")
            
            # Charger les stations depuis la base SQLite (321 uniques)
            with sqlite3.connect(self.db_path) as conn:
                self.stops = pd.read_sql_query("SELECT * FROM stops", conn)
            self.routes = pd.read_pickle(f"{self.data_path}/routes.pkl")
            self.transfers = pd.read_pickle(f"{self.data_path}/transfers.pkl")
            logger.info(f"Donn√©es charg√©es: {len(self.stops)} stations (uniques), {len(self.routes)} lignes, {len(self.transfers)} correspondances")

            
            # V√©rifier si la DB existe et est √† jour
            if not os.path.exists(self.db_path) or self._db_needs_update():
                logger.info("Cr√©ation/mise √† jour de la base de donn√©es...")
                self._create_database()
            else:
                logger.info("Base de donn√©es existante trouv√©e et √† jour")
                
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation: {e}")
            raise
    
    def _db_needs_update(self) -> bool:
        """V√©rifie si la DB doit √™tre mise √† jour"""
        try:
            # Comparer les timestamps des fichiers
            db_mtime = os.path.getmtime(self.db_path)
            pickle_files = [
                f"{self.data_path}/stops.pkl",
                f"{self.data_path}/stop_times.pkl",
                f"{self.data_path}/trips.pkl"
            ]
            
            for file in pickle_files:
                if os.path.exists(file) and os.path.getmtime(file) > db_mtime:
                    return True
            return False
        except:
            return True
    
    def _create_database(self):
        """Cr√©e la base de donn√©es SQLite optimis√©e"""
        logger.info("Cr√©ation de la base de donn√©es SQLite...")
        
        # Supprimer l'ancienne DB si elle existe
        if os.path.exists(self.db_path):
            os.remove(self.db_path)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # Table des stations avec index spatial
            cursor.execute('''
                CREATE TABLE stops (
                    stop_id TEXT PRIMARY KEY,
                    stop_name TEXT,
                    stop_lat REAL,
                    stop_lon REAL,
                    wheelchair_boarding INTEGER,
                    zone_id TEXT
                )
            ''')
            
            # Index spatial sur les coordonn√©es
            cursor.execute('CREATE INDEX idx_stops_location ON stops(stop_lat, stop_lon)')
            
            # Table des ar√™tes pr√©-calcul√©es
            cursor.execute('''
                CREATE TABLE edges (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    from_stop_id TEXT,
                    to_stop_id TEXT,
                    route_id TEXT,
                    trip_id TEXT,
                    travel_time REAL,
                    route_short_name TEXT,
                    FOREIGN KEY (from_stop_id) REFERENCES stops(stop_id),
                    FOREIGN KEY (to_stop_id) REFERENCES stops(stop_id)
                )
            ''')
            
            # Index sur les ar√™tes
            cursor.execute('CREATE INDEX idx_edges_from_stop ON edges(from_stop_id)')
            cursor.execute('CREATE INDEX idx_edges_to_stop ON edges(to_stop_id)')
            cursor.execute('CREATE INDEX idx_edges_route ON edges(route_id)')
            
            # Table des transferts
            cursor.execute('''
                CREATE TABLE transfers (
                    from_stop_id TEXT,
                    to_stop_id TEXT,
                    transfer_time REAL,
                    PRIMARY KEY (from_stop_id, to_stop_id)
                )
            ''')
            
            # Ins√©rer les stations
            logger.info("Insertion des stations...")
            stops_data = []
            for _, stop in self.stops.iterrows():
                stops_data.append((
                    stop['stop_id'],
                    stop['stop_name'],
                    stop['stop_lat'],
                    stop['stop_lon'],
                    stop.get('wheelchair_boarding', 0),
                    stop.get('zone_id', '')
                ))
            
            cursor.executemany('''
                INSERT INTO stops (stop_id, stop_name, stop_lat, stop_lon, wheelchair_boarding, zone_id)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', stops_data)
            
            # Ins√©rer les transferts
            logger.info("Insertion des transferts...")
            transfers_data = []
            for _, transfer in self.transfers.iterrows():
                transfers_data.append((
                    transfer['from_stop_id'],
                    transfer['to_stop_id'],
                    transfer.get('min_transfer_time', 120)
                ))
            
            cursor.executemany('''
                INSERT INTO transfers (from_stop_id, to_stop_id, transfer_time)
                VALUES (?, ?, ?)
            ''', transfers_data)
            
            # Traiter les ar√™tes en parall√®le
            logger.info("Traitement des ar√™tes en parall√®le...")
            self._process_edges_parallel(conn)
            
            conn.commit()
            logger.info("Base de donn√©es cr√©√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"Erreur lors de la cr√©ation de la DB: {e}")
            conn.rollback()
            raise
        finally:
            conn.close()
    
    def _process_edges_parallel(self, conn):
        """Traite les ar√™tes en parall√®le pour optimiser les performances"""
        
        # Charger les donn√©es n√©cessaires
        trips = pd.read_pickle(f"{self.data_path}/trips.pkl")
        stop_times = pd.read_pickle(f"{self.data_path}/stop_times.pkl")
        
        # Cr√©er un mapping route -> short_name
        route_names = self.routes.set_index('route_id')['route_short_name'].to_dict()
        
        # Diviser les trips en chunks pour le traitement parall√®le
        chunk_size = 1000
        trip_chunks = [trips[i:i+chunk_size] for i in range(0, len(trips), chunk_size)]
        
        logger.info(f"Traitement de {len(trips)} trips en {len(trip_chunks)} chunks...")
        
        all_edges = []
        
        # Traiter chaque chunk
        for i, chunk in enumerate(trip_chunks):
            if i % 10 == 0:
                logger.info(f"Traitement du chunk {i+1}/{len(trip_chunks)}")
            
            chunk_edges = self._process_trip_chunk(chunk, stop_times, route_names)
            all_edges.extend(chunk_edges)
        
        # Ins√©rer toutes les ar√™tes
        logger.info(f"Insertion de {len(all_edges)} ar√™tes...")
        cursor = conn.cursor()
        cursor.executemany('''
            INSERT INTO edges (from_stop_id, to_stop_id, route_id, trip_id, travel_time, route_short_name)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', all_edges)
        
        logger.info("Ar√™tes ins√©r√©es avec succ√®s")
    
    def _process_trip_chunk(self, trip_chunk, stop_times, route_names):
        """Traite un chunk de trips pour extraire les ar√™tes"""
        edges = []
        
        for _, trip in trip_chunk.iterrows():
            trip_id = trip['trip_id']
            route_id = trip['route_id']
            route_short_name = route_names.get(route_id, '')
            
            # Obtenir les stop_times pour ce trip
            trip_stops = stop_times[stop_times['trip_id'] == trip_id].sort_values('stop_sequence')
            
            if len(trip_stops) < 2:
                continue
            
            # Cr√©er les ar√™tes cons√©cutives
            for i in range(len(trip_stops) - 1):
                current_stop = trip_stops.iloc[i]
                next_stop = trip_stops.iloc[i + 1]
                
                # Calculer le temps de trajet
                try:
                    dep_time = self._parse_time(current_stop['departure_time'])
                    arr_time = self._parse_time(next_stop['arrival_time'])
                    travel_time = (arr_time - dep_time).total_seconds()
                    travel_time = max(travel_time, 60)  # Minimum 1 minute
                except:
                    travel_time = 120  # Valeur par d√©faut
                
                edges.append((
                    current_stop['stop_id'],
                    next_stop['stop_id'],
                    route_id,
                    trip_id,
                    travel_time,
                    route_short_name
                ))
        
        return edges
    
    def _parse_time(self, time_str: str) -> datetime:
        """Parse un temps GTFS optimis√©"""
        try:
            hours, minutes, seconds = map(int, time_str.split(':'))
            if hours >= 24:
                hours -= 24
                return datetime.strptime(f"{hours:02d}:{minutes:02d}:{seconds:02d}", "%H:%M:%S") + timedelta(days=1)
            return datetime.strptime(time_str, "%H:%M:%S")
        except:
            return datetime.strptime("00:00:00", "%H:%M:%S")
    
    def build_graph_for_zone_sql(self, lat_min: float, lat_max: float, lon_min: float, lon_max: float) -> nx.DiGraph:
        """
        Construit un graphe pour une zone en utilisant SQLite pour des performances optimales.
        """
        logger.info(f"Construction du graphe SQL pour zone: [{lat_min},{lat_max}] x [{lon_min},{lon_max}]")
        
        conn = sqlite3.connect(self.db_path)
        
        try:
            # Requ√™te optimis√©e pour obtenir les stations dans la zone
            stations_query = '''
                SELECT stop_id, stop_name, stop_lat, stop_lon, wheelchair_boarding, zone_id
                FROM stops
                WHERE stop_lat BETWEEN ? AND ? AND stop_lon BETWEEN ? AND ?
            '''
            
            stations_df = pd.read_sql_query(
                stations_query, 
                conn, 
                params=[lat_min, lat_max, lon_min, lon_max]
            )
            
            if stations_df.empty:
                logger.warning("Aucune station dans la zone")
                return nx.DiGraph()
            
            logger.info(f"üöá Trouv√© {len(stations_df)} stations dans la zone")
            
            # Cr√©er le graphe
            graph = nx.DiGraph()
            
            # Ajouter les n≈ìuds
            for _, station in stations_df.iterrows():
                graph.add_node(
                    station['stop_id'],
                    stop_name=station['stop_name'],
                    lat=station['stop_lat'],
                    lon=station['stop_lon'],
                    accessibility=station['wheelchair_boarding']
                )
            
            # Obtenir les ar√™tes dans la zone
            stop_ids = list(stations_df['stop_id'])
            placeholders = ','.join(['?' for _ in stop_ids])
            
            edges_query = f'''
                SELECT from_stop_id, to_stop_id, travel_time, route_short_name, route_id
                FROM edges
                WHERE from_stop_id IN ({placeholders}) AND to_stop_id IN ({placeholders})
            '''
            
            edges_df = pd.read_sql_query(edges_query, conn, params=stop_ids + stop_ids)
            
            logger.info(f"üöá Trouv√© {len(edges_df):,} ar√™tes de m√©tro dans la zone")
            
            # Ajouter les ar√™tes
            for _, edge in edges_df.iterrows():
                if edge['from_stop_id'] in graph.nodes and edge['to_stop_id'] in graph.nodes:
                    graph.add_edge(
                        edge['from_stop_id'],
                        edge['to_stop_id'],
                        weight=edge['travel_time'],
                        route_id=edge['route_id'],
                        route_name=edge['route_short_name']
                    )
            
            # Ajouter les transferts
            transfers_query = f'''
                SELECT from_stop_id, to_stop_id, transfer_time
                FROM transfers
                WHERE from_stop_id IN ({placeholders}) AND to_stop_id IN ({placeholders})
            '''
            
            transfers_df = pd.read_sql_query(transfers_query, conn, params=stop_ids + stop_ids)
            
            logger.info(f"üîÑ Ajout de {len(transfers_df)} correspondances (transferts entre lignes)")
            
            for _, transfer in transfers_df.iterrows():
                if transfer['from_stop_id'] in graph.nodes and transfer['to_stop_id'] in graph.nodes:
                    graph.add_edge(
                        transfer['from_stop_id'],
                        transfer['to_stop_id'],
                        weight=transfer['transfer_time'],
                        transfer_type='correspondence'
                    )
            
            logger.info(f"‚úÖ Graphe SQL final: {len(graph.nodes)} stations, {len(graph.edges)} connexions total")
            return graph
            
        except Exception as e:
            logger.error(f"Erreur lors de la construction du graphe SQL: {e}")
            raise
        finally:
            conn.close()
    
    def build_stations_only_graph(self) -> nx.DiGraph:
        """
        Construit un graphe avec seulement les stations (ultra-rapide).
        """
        logger.info("üèóÔ∏è  Construction du graphe stations uniquement (sans connexions)...")
        
        graph = nx.DiGraph()
        
        # Ajouter toutes les stations comme n≈ìuds
        for _, stop in self.stops.iterrows():
            graph.add_node(
                stop['stop_id'],
                stop_name=stop['stop_name'],
                lat=stop['stop_lat'],
                lon=stop['stop_lon'],
                accessibility=stop.get('wheelchair_boarding', 0)
            )
        
        logger.info(f"‚úÖ Graphe stations cr√©√©: {len(graph.nodes)} stations de m√©tro")
        return graph
    
    @lru_cache(maxsize=128)
    def get_route_info(self, route_id: str) -> Optional[Dict]:
        """Obtient les informations d'une ligne (avec cache)"""
        route_data = self.routes[self.routes['route_id'] == route_id]
        if not route_data.empty:
            return route_data.iloc[0].to_dict()
        return None
    
    def get_statistics(self) -> Dict:
        """Obtient des statistiques sur le syst√®me"""
        conn = sqlite3.connect(self.db_path)
        
        try:
            stats = {}
            
            # Statistiques de base
            # Toujours compter les stations depuis la table stops SQLite (321 uniques)
            cursor.execute('SELECT COUNT(*) FROM stops')
            stats['total_stations'] = cursor.fetchone()[0]
            stats['total_routes'] = len(self.routes)
            stats['total_transfers'] = len(self.transfers)
            
            # Statistiques depuis la DB
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM edges')
            stats['total_edges'] = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT route_short_name, COUNT(*) as edge_count
                FROM edges
                GROUP BY route_short_name
                ORDER BY edge_count DESC
            ''')
            
            stats['edges_by_route'] = dict(cursor.fetchall())
            
            return stats
            
        finally:
            conn.close()
    
    def clear_database(self):
        """Supprime la base de donn√©es pour forcer une reconstruction"""
        if os.path.exists(self.db_path):
            os.remove(self.db_path)
            logger.info("Base de donn√©es supprim√©e - prochaine utilisation recr√©era la DB")
        else:
            logger.info("Aucune base de donn√©es √† supprimer")
    
    def build_full_network_graph(self) -> nx.DiGraph:
        """
        Construit un graphe complet avec toutes les stations et connexions du r√©seau.
        Version optimis√©e qui √©vite de recalculer si d√©j√† fait.
        """
        logger.info("Construction du graphe complet pour v√©rification de connexit√©...")
        
        conn = sqlite3.connect(self.db_path)
        
        try:
            # Cr√©er le graphe
            graph = nx.DiGraph()
            
            # Ajouter toutes les stations
            logger.info("Ajout de toutes les stations...")
            for _, stop in self.stops.iterrows():
                graph.add_node(
                    stop['stop_id'],
                    stop_name=stop['stop_name'],
                    lat=stop['stop_lat'],
                    lon=stop['stop_lon'],
                    accessibility=stop.get('wheelchair_boarding', 0)
                )
            
            # V√©rifier si la base contient des ar√™tes
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM edges')
            edges_count = cursor.fetchone()[0]
            
            if edges_count == 0:
                logger.warning("‚ö†Ô∏è Aucune ar√™te dans la base, construction rapide avec transferts uniquement...")
                # Utiliser seulement les transferts pour un test rapide
                for _, transfer in self.transfers.iterrows():
                    if transfer['from_stop_id'] in graph.nodes and transfer['to_stop_id'] in graph.nodes:
                        # Ajouter les transferts dans les deux sens
                        graph.add_edge(
                            transfer['from_stop_id'],
                            transfer['to_stop_id'],
                            weight=transfer.get('min_transfer_time', 120),
                            edge_type='transfer'
                        )
                        graph.add_edge(
                            transfer['to_stop_id'],
                            transfer['from_stop_id'],
                            weight=transfer.get('min_transfer_time', 120),
                            edge_type='transfer'
                        )
                
                logger.info(f"‚úÖ Graphe rapide cr√©√©: {len(graph.nodes)} stations, {len(graph.edges)} connexions (transferts uniquement)")
                logger.warning("‚ö†Ô∏è ATTENTION: Ce graphe ne contient que les transferts. Pour une connexit√© compl√®te, la base de donn√©es doit contenir les ar√™tes m√©tro.")
                return graph
            
            # Ajouter toutes les ar√™tes de m√©tro
            logger.info("Ajout de toutes les connexions m√©tro...")
            edges_df = pd.read_sql_query('''
                SELECT from_stop_id, to_stop_id, travel_time, route_short_name, route_id
                FROM edges
            ''', conn)
            
            for _, edge in edges_df.iterrows():
                if edge['from_stop_id'] in graph.nodes and edge['to_stop_id'] in graph.nodes:
                    graph.add_edge(
                        edge['from_stop_id'],
                        edge['to_stop_id'],
                        weight=edge['travel_time'],
                        route_id=edge['route_id'],
                        route_name=edge['route_short_name'],
                        edge_type='metro'
                    )
            
            # Ajouter tous les transferts
            logger.info("Ajout de toutes les correspondances...")
            transfers_df = pd.read_sql_query('''
                SELECT from_stop_id, to_stop_id, transfer_time
                FROM transfers
            ''', conn)
            
            for _, transfer in transfers_df.iterrows():
                if transfer['from_stop_id'] in graph.nodes and transfer['to_stop_id'] in graph.nodes:
                    # Ajouter le transfert dans les deux sens pour la connexit√©
                    graph.add_edge(
                        transfer['from_stop_id'],
                        transfer['to_stop_id'],
                        weight=transfer['transfer_time'],
                        edge_type='transfer'
                    )
                    graph.add_edge(
                        transfer['to_stop_id'],
                        transfer['from_stop_id'],
                        weight=transfer['transfer_time'],
                        edge_type='transfer'
                    )
            
            logger.info(f"‚úÖ Graphe complet cr√©√©: {len(graph.nodes)} stations, {len(graph.edges)} connexions")
            return graph
            
        except Exception as e:
            logger.error(f"Erreur lors de la construction du graphe complet: {e}")
            raise
        finally:
            conn.close()
    
    def connected(self):
        """
        V√©rifie si le r√©seau de transport est connexe (toutes les stations sont accessibles).
        Pour un graphe orient√©, on v√©rifie la connexit√© faible.
        """
        logger.info("üîç V√©rification de la connexit√© du r√©seau...")
        
        # Construire un graphe avec toutes les stations et connexions
        if not hasattr(self, '_full_graph') or self._full_graph is None:
            self._full_graph = self.build_full_network_graph()
        
        # Pour un r√©seau de transport, on v√©rifie la connexit√© faible
        # (ignore la direction des ar√™tes)
        is_connected = nx.is_weakly_connected(self._full_graph)
        
        logger.info(f"üìä R√©seau connexe: {'‚úÖ Oui' if is_connected else '‚ùå Non'}")
        return is_connected
    
    def get_connectivity_details(self) -> Dict:
        """
        Retourne des d√©tails sur la connexit√© du r√©seau.
        """
        logger.info("üîç Analyse d√©taill√©e de la connexit√©...")
        
        if not hasattr(self, '_full_graph') or self._full_graph is None:
            self._full_graph = self.build_full_network_graph()
        
        details = {
            'is_connected': nx.is_weakly_connected(self._full_graph),
            'total_nodes': len(self._full_graph.nodes),
            'total_edges': len(self._full_graph.edges),
            'number_of_components': nx.number_weakly_connected_components(self._full_graph),
            'largest_component_size': 0,
            'isolated_nodes': [],
            'components_info': []
        }
        
        # Analyser les composantes connexes
        components = list(nx.weakly_connected_components(self._full_graph))
        components.sort(key=len, reverse=True)
        
        details['largest_component_size'] = len(components[0]) if components else 0
        
        # Identifier les n≈ìuds isol√©s (composantes de taille 1)
        for component in components:
            if len(component) == 1:
                node_id = list(component)[0]
                node_data = self._full_graph.nodes[node_id]
                details['isolated_nodes'].append({
                    'stop_id': node_id,
                    'stop_name': node_data.get('stop_name', 'Inconnu'),
                    'lat': node_data.get('lat', 0),
                    'lon': node_data.get('lon', 0)
                })
        
        # Informations sur toutes les composantes
        for i, component in enumerate(components):
            component_info = {
                'component_id': i + 1,
                'size': len(component),
                'percentage': (len(component) / details['total_nodes']) * 100 if details['total_nodes'] > 0 else 0
            }
            details['components_info'].append(component_info)
        
        logger.info(f"üìä Analyse termin√©e: {details['number_of_components']} composante(s), plus grande: {details['largest_component_size']} stations")
        
        return details


    def get_shortest_path(self, from_stop_id: str, to_stop_id: str) -> Optional[Dict]:
        """
        Calcule le plus court chemin entre deux arr√™ts en utilisant le poids (temps de trajet ou de transfert).
        """
        logger.info(f"üîç Calcul du plus court chemin entre {from_stop_id} et {to_stop_id}...")

        # Assure-toi que le graphe complet est bien construit
        if not hasattr(self, '_full_graph') or self._full_graph is None:
            self._full_graph = self.build_full_network_graph()

        G = self._full_graph

        logger.info(f"üß© Le graphe contient {len(G.nodes)} n≈ìuds.")
        logger.info(f"üß© N≈ìuds disponibles (stop_id): {list(G.nodes)[:20]} ...")  # Affiche les 20 premiers
        if from_stop_id not in G or to_stop_id not in G:
            logger.warning("Un ou les deux arr√™ts ne sont pas pr√©sents dans le graphe.")
            return None

        try:
            path = nx.astar_path(G, source=from_stop_id, target=to_stop_id, weight='weight')
            total_weight = nx.astar_path_length(G, source=from_stop_id, target=to_stop_id, weight='weight')

            # Extraire les ar√™tes parcourues
            edges = []
            for i in range(len(path) - 1):
                edge_data = G.get_edge_data(path[i], path[i + 1])
                edges.append({
                    'from': path[i],
                    'to': path[i + 1],
                    'weight': edge_data.get('weight'),
                    'route': edge_data.get('route_name', ''),
                    'type': edge_data.get('edge_type', 'metro')
                })

            return {
                'path': path,
                'edges': edges,
                'total_travel_time': total_weight
            }
        except nx.NetworkXNoPath:
            logger.warning(f"Aucun chemin trouv√© entre {from_stop_id} et {to_stop_id}")
            return None

